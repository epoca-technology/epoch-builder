from typing import Literal, TypedDict, Union, List, Tuple






#####################
## Model Templates ##
#####################



# Name of the variations within each network
IKerasModelTemplateName = Literal[
    "KR_DNN_S2", "KR_DNN_S3", "KR_DNN_S4", "KR_DNN_S5",
    "KR_CDNN_S2", "KR_CDNN_S3", "KR_CDNN_S4", "KR_CDNN_S5",
    "KR_CDNN_MP_S2", "KR_CDNN_MP_S3", "KR_CDNN_MP_S4", "KR_CDNN_MP_S5",
    "KR_LSTM_S2", "KR_LSTM_S3", "KR_LSTM_S4", "KR_LSTM_S5",
    "KR_CLSTM_S2", "KR_CLSTM_S3", "KR_CLSTM_S4", "KR_CLSTM_S5",
    "KR_CLSTM_MP_S2", "KR_CLSTM_MP_S3", "KR_CLSTM_MP_S4", "KR_CLSTM_MP_S5"
]









#########################
## Model Configuration ##
#########################


# Units used for Dense and LSTM layers
IKerasUnit = Literal[2, 4, 8, 16, 32, 64, 128, 256, 512, 1024]

# Filters used in CNN layers
IKerasFilter = Literal[2, 4, 8, 16, 32, 64, 128, 256, 512, 1024]

# Kernel Size used in CNN layers
IKerasKernelSize = Literal[3, 5]

# Pool Size used in CNN_MP layers
IKerasPoolSize = Literal[2, 4]


# Optimizer Functions
IKerasOptimizer = Literal["adam", "rmsprop"]


# Loss Functions
IKerasLoss = Literal["mean_absolute_error", "mean_squared_error"]


# Metric Functions
IKerasMetric = Literal["mean_absolute_error", "mean_squared_error"]


# Activation Functions
IKerasActivation = Literal["relu", "tanh"]





# Keras Model Configuration
# The configuration that will be used to build the Keras Model.
class IKerasModelConfig(TypedDict):
    # The name of the Keras Model. If it doesn't exist it will raise an error.
    name: IKerasModelTemplateName

    # Units
    units: Union[List[IKerasUnit], None]

    # Dropout rates
    dropout_rates: Union[List[float], None]

    # Activations
    activations: Union[List[str], List[IKerasActivation], None]

    # Filters
    filters: Union[List[IKerasFilter], None]

    # Kernel Sizes
    kernel_sizes: Union[List[IKerasKernelSize], None]

    # Pool Sizes
    pool_sizes: Union[List[IKerasPoolSize], None]

    # Input: The number of past candlesticks the model looks at in order to generate
    # a prediction
    lookback: Union[int, None]

    # Output: The number of predictions that will be generated by the model
    predictions: Union[int, None]













############################
## Training Configuration ##
############################



# Keras Training Configuration
# The configuration dict that will be used to assist the Regression Training process.
# For more information regarding these args, view the LearningRateSchedule.ipynb notebook.
class IKerasTrainingConfig(TypedDict):
    # A scalar float32 or float64 Tensor or a Python number. The initial learning rate.
    initial_lr: float

    # How often to apply decay.
    decay_steps: float

    # A Python number. The decay rate for the learning rate per step.
    decay_rate: float

    # The maximum number of epochs the training process will go through
    max_epochs: int

    # Number of epochs with no improvement after which training will be stopped.
    patience: int

    # Number of samples per gradient update. If unspecified, batch_size will default to 32. 
    # Do not specify the batch_size if your data is in the form of datasets
    batch_size: int














######################
## Training History ##
######################



# Training History
# The dictionary built once the training is completed. The properties adapt accordingly 
# based on the loss and metric functions used.
class IKerasModelTrainingHistory(TypedDict):
    # Training and validation loss
    loss: List[float]
    val_loss: List[float]

    # Regression Values
    mean_absolute_error: Union[List[float], None]
    val_mean_absolute_error: Union[List[float], None]
    mean_squared_error: Union[List[float], None]
    val_mean_squared_error: Union[List[float], None]













###################
## Model Summary ##
###################




# Optimizer Name
IKerasOptimizerName = Literal["Adam", "RMSprop"]





# Model Optimizer Config
# The optimizer configuration used when the model was compiled. Keep in mind that
# all these values are stringified to ensure compatibility with the JSON file format.
class IKerasModelOptimizerConfig(TypedDict):
    name: IKerasOptimizerName
    learning_rate: str
    decay: Union[str, None]
    beta_1: Union[str, None]
    beta_2: Union[str, None]
    epsilon: Union[str, None]
    amsgrad: Union[str, None]
    rho: Union[str, None]
    momentum: Union[str, None]
    centered: Union[str, None]







# Model Loss Config
# The loss configuration used when the model was compiled. Keep in mind that
# all these values are stringified to ensure compatibility with the JSON file format.
class IKerasModelLossConfig(TypedDict):
    name: IKerasLoss
    reduction: Union[str, None]
    from_logits: Union[str, None]
    label_smoothing: Union[str, None]
    axis: Union[str, None]




# Model Layer
# A layer stacked with other layers within the model.
class IKerasModelLayer(TypedDict):
    name: str
    params: int
    input_shape: Tuple[Union[int, None]]
    output_shape: Tuple[Union[int, None]]
    trainable: bool




# Model Class Name
IKerasModelClassName = Literal["Sequential"]


# Metric Names
IKerasMetricName = Literal["loss"]


# Model Summary
# Extracts all the relevant information from a trained model.
class IKerasModelSummary(TypedDict):
    # The name of the class used to create the model.
    model_class: IKerasModelClassName

    # Optimizer Config
    optimizer_config: IKerasModelOptimizerConfig

    # Loss Config
    loss_config: IKerasModelLossConfig

    # Metrics
    metrics: List[IKerasMetricName]

    # Input and output shapes
    input_shape: Tuple[Union[int, None]]
    output_shape: Tuple[Union[int, None]]

    # Layers
    layers: List[IKerasModelLayer]

    # Params
    total_params: int
    trainable_params: int
    non_trainable_params: int









